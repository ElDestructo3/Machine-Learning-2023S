{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                 # pandas used only to read the initial table\n",
    "import numpy as np                                                  # numpy used only for linear algebra operations                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ipl.csv')                                         # read the csv file                  \n",
    "#print(df.head())                                                          # uncomment to see the first 5 rows of the table                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)                                                         # uncomment to check the columns of the table                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing:\n",
    "# Step 1:  converting '-' values to 0\n",
    "# Step 2: droppng player names as it is irrelevant for clustering, and also dropping the target variable 'y' and 'BBI' as they have 0 values for all the players\n",
    "# Step 3: normalizing data using z-score normalization\n",
    "df_names = df['PLAYER']\n",
    "df_replace = df.drop('PLAYER', axis=1)\n",
    "df_replace = df_replace.drop( ['y', 'BBI'], axis=1)\n",
    "df_replace.replace('-', 0, inplace=True)\n",
    "df_replace = df_replace.apply(pd.to_numeric, errors='coerce')                 # convert all the values to numeric to normalize the data   \n",
    "df_replace = (df_replace - df_replace.mean()) / df_replace.std()\n",
    "df_replace.replace(np.nan, 0, inplace=True)\n",
    "X = df_replace.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaion of K-Means Clustering as a class\n",
    "\n",
    "class MyKMeans(object):\n",
    "    '''\n",
    "    Class variables:\n",
    "    k: number of clusters\n",
    "    iterations: number of iterations to run the algorithm, default set to 20\n",
    "    X: input data\n",
    "    centroids: centroids of the clusters\n",
    "    clusters: list of lists, each list contains the indices of the points in the cluster\n",
    "    clustered_points: list of lists, each list contains the points in the cluster\n",
    "    cluster_indices: list of integers, each integer represents the cluster index of the corresponding point in X\n",
    "\n",
    "    Functions:\n",
    "    __init__: initializes the class variables\n",
    "    cosine_similarity_distance: calculates the cosine similarity distance between two vectors\n",
    "    fit: applies k-means clustering on the input data X\n",
    "    silhouette_score: evaluates the clustering using the silhouette score metric after the clustering has been done using fit()\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k, iterations = 20):\n",
    "\n",
    "\n",
    "        '''\n",
    "        Initialize the class variables\n",
    "        '''\n",
    "\n",
    "        self.iterations = iterations\n",
    "        self.k = k\n",
    "\n",
    "    def cosine_similarity_distance(self, a, b):\n",
    "        '''\n",
    "        Returns the cosine similarity distance between two vectors a and b\n",
    "        '''\n",
    "\n",
    "        return 1 - np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        '''\n",
    "        Applies k-means clustering on the input data X.\n",
    "\n",
    "        k centroids are randomly chosen from the data. A number of iterations are run until the maximum number of iterations\n",
    "        is reached or the centroids do not change. In each iteration, point is moved to the cluster with the closest centroid,\n",
    "        calculated using cosine similarity distance. The centroid of each cluster is then recalculated by taking the mean of\n",
    "        all the points in each cluster.\n",
    "\n",
    "        The data is then once again used using the final centroids to give the final clusters and their indices, which are\n",
    "        stored in the class variables and also returned by the function as (clusters, cluster_indices)\n",
    "        '''\n",
    "\n",
    "        self.X = X\n",
    "        self.centroids = {}\n",
    "        self.cluster_indices = []\n",
    "        self.clusters = []\n",
    "        self.clustered_points = []\n",
    "        idx = np.random.choice(len(X), self.k, replace=False)\n",
    "        self.centroids = X[idx]\n",
    "        #print(self.centroids)\n",
    "        last_centroids = np.zeros(self.centroids.shape)\n",
    "        iters = 0\n",
    "        while np.not_equal(self.centroids, last_centroids).any() and iters < self.iterations:\n",
    "            clustered_points = [[] for i in range(self.k)]\n",
    "            for point in X:\n",
    "                distances = [self.cosine_similarity_distance(point, centroid) for centroid in self.centroids]\n",
    "                #print(distances)\n",
    "                clustered_points[np.argmin(distances)].append(point)                        # add the point to the cluster with the closest centroid\n",
    "            last_centroids = self.centroids\n",
    "            self.centroids = [np.mean(cluster, axis=0) for cluster in clustered_points]\n",
    "            for i in range(self.k):\n",
    "                if len(clustered_points[i]) == 0:                                           # if a cluster is empty, set the centroid to the last centroid\n",
    "                    self.centroids[i] = last_centroids[i]\n",
    "            iters += 1\n",
    "        clustered_points = [[] for i in range(self.k)]\n",
    "        cluster_indices = []\n",
    "        for point in X:\n",
    "            distances = [self.cosine_similarity_distance(point, centroid) for centroid in self.centroids]\n",
    "            clustered_points[np.argmin(distances)].append(point)\n",
    "            cluster_indices.append(np.argmin(distances))\n",
    "        self.clusters = [[] for i in range(self.k)]\n",
    "        for i in range(len(cluster_indices)):\n",
    "            self.clusters[cluster_indices[i]].append(i)                                     # store the indices of the points in the clusters\n",
    "        self.clustered_points = clustered_points\n",
    "        self.cluster_indices = cluster_indices\n",
    "        return self.clusters, cluster_indices\n",
    "    \n",
    "    def silhouette_score(self):\n",
    "        '''\n",
    "        Returns the clustering using the silhouette score metric after the clustering has been done using fit().\n",
    "\n",
    "        List a is evaluated as the average distance between a point and all the other points in the same cluster,\n",
    "        taking care of the null case by using mean as 0. List b is evaluated as the average distance between a point\n",
    "        and all the other points in the nearest cluster, which is decided by finding cluster with minimum average \n",
    "        distance to the point.\n",
    "         \n",
    "        The silhouette score list s_arr is then calculated as (b - a)/max(a, b) for each point and the final score\n",
    "        s is the mean of all the values in s_arr.\n",
    "        '''\n",
    "\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(self.X)):\n",
    "            ai = np.array([self.cosine_similarity_distance(self.X[i], self.X[j]) for j in range(len(self.X)) if i != j and self.cluster_indices[i] == self.cluster_indices[j]])\n",
    "            a.append(np.mean(ai) if len(ai) > 0 else 0)\n",
    "            if np.isnan(a[-1]):\n",
    "                a[-1] = 1e9\n",
    "    \n",
    "        for i in range(len(self.X)):\n",
    "            bi = []\n",
    "            for n in range(self.k):\n",
    "                if n == self.cluster_indices[i]:\n",
    "                    continue\n",
    "                temp = [self.cosine_similarity_distance(self.X[i], self.X[j]) for j in range(len(self.X)) if i != j and self.cluster_indices[j] == n]\n",
    "                if len(temp) == 0:\n",
    "                    temp = [1e9]\n",
    "                bi.append(np.mean(temp))\n",
    "            b.append(min(bi))\n",
    "            if np.isnan(b[-1]):\n",
    "                b[-1] = 1e9\n",
    "        \n",
    "        s_arr = [(b[i] - a[i]) / (max(a[i], b[i])) for i in range(len(self.X))]\n",
    "        s = np.mean(s_arr)\n",
    "        return s\n",
    "    \n",
    "    def save_file(self, file_name):\n",
    "        '''\n",
    "        Saves the clusters in a file with the name file_name.\n",
    "        \n",
    "        Each individual cluster is sorted and then the clusters are sorted according to the first element of each cluster\n",
    "        (which gives the minimum indexed point in the cluster). These clusters are then saved in the file, with each cluster\n",
    "        on a new line and all the cluster indices in the cluster separated by a comma.\n",
    "        '''\n",
    "\n",
    "        temp_clusters = [sorted(self.clusters[i]) for i in range(len(self.clusters))]\n",
    "        min_vals = [[temp_clusters[i][0], i] for i in range(len(temp_clusters))]         # stores minimum value of each cluster and its index in the original list\n",
    "        min_vals = sorted(min_vals, key=lambda x: x[0])\n",
    "        final_clusters = []\n",
    "        for i in range(len(temp_clusters)):\n",
    "            final_clusters.append(temp_clusters[min_vals[i][1]])\n",
    "        with open(file_name, 'w') as f:\n",
    "            for i in range(len(final_clusters)):\n",
    "                for j in range(len(final_clusters[i])):\n",
    "                    f.write(str(final_clusters[i][j]) + ', ')\n",
    "                f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for k = 3 is 0.5029974515142861\n",
      "Silhouette score for k = 4 is 0.48235697150455253\n",
      "Silhouette score for k = 5 is 0.4300800819305617\n",
      "Silhouette score for k = 6 is 0.4906898647818525\n",
      "Model with best silhouette score for k = 3 has score 0.5029974515142861\n"
     ]
    }
   ],
   "source": [
    "# Finding best value of k using KMeans\n",
    "\n",
    "best_k = -1\n",
    "best_score_kmeans = -1\n",
    "best_model_kmeans = None\n",
    "for k in range(3, 7):\n",
    "   \n",
    "    kmeans = MyKMeans(k)\n",
    "    kmeans.fit(X)\n",
    "    score = kmeans.silhouette_score()\n",
    "    print('Silhouette score for k = {} is {}'.format(k, score))\n",
    "    if score > best_score_kmeans:\n",
    "        best_score_kmeans = score\n",
    "        best_model_kmeans = kmeans\n",
    "        best_k = k\n",
    "    \n",
    "print('Model with best silhouette score for k = {} has score {}'.format(best_k, best_score_kmeans))\n",
    "    \n",
    "\n",
    "# testing implemntation of silhouette score\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# km = MyKMeans(3)\n",
    "# km.fit(X)\n",
    "# km.evaluate(X)\n",
    "# print(silhouette_score(X, km.cluster_indices))\n",
    "# km.silhouette_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemenation of Single Linkage Top Down Clustering\n",
    "\n",
    "class singleLinkageTopDown(object):\n",
    "    '''\n",
    "    Class variables:\n",
    "    X: Input data\n",
    "    k: The number of clusters\n",
    "    clusters: list of clusters, each cluster is a list of indices of the points in the cluster\n",
    "    distances: matrix of pairwise distances between all the points in the input data, calculated using cosine similarity\n",
    "    cluster_indices: list of cluster indices of each point in the input data\n",
    "\n",
    "    Functions:\n",
    "    __init__: initializes the class variables\n",
    "    cosine_similarity_distance: calculates the cosine similarity distance between two vectors\n",
    "    cluster_diameter: calculates the diameter of a cluster (the maximum distance between two points in the cluster)\n",
    "    single_linkage: calculates the single linkage distance between two clusters\n",
    "    complete_linkage: calculates the complete linkage distance between two clusters\n",
    "    split_point: Finds the point about which a cluster is to be split\n",
    "    split_cluster: Splits a cluster into two clusters using the extra point\n",
    "    fit: Performs the clustering\n",
    "    silhouette_score: Evaluates the clustering using the silhouette score metric\n",
    "    save_file: Saves the clusters in a file \n",
    "    '''\n",
    "\n",
    "    def __init__(self, k):\n",
    "        '''\n",
    "        Initializes the class variable k (number of clusters).\n",
    "        '''\n",
    "\n",
    "        self.k = k\n",
    "        self.X = None\n",
    "        self.clusters = None\n",
    "        self.distances = None\n",
    "        self.cluster_indices = None\n",
    "\n",
    "    def cosine_similarity_distance(self, a, b):\n",
    "        '''\n",
    "        Returns the cosine similarity distance between two vectors a and b\n",
    "        '''\n",
    "\n",
    "        return 1 - np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    \n",
    "    \n",
    "    def cluster_diameter(self, cluster):\n",
    "        '''\n",
    "        Returns the diameter of cluster.\n",
    "        \n",
    "        The diameter of a cluster is the maximum distance between any two points in the cluster.\n",
    "        '''\n",
    "\n",
    "        max_dist = 0\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i+1, len(cluster)):\n",
    "                max_dist = max(max_dist, self.distances[cluster[i]][cluster[j]])\n",
    "        return max_dist\n",
    "    \n",
    "    def single_linkage(self, cluster1, cluster2):\n",
    "        '''\n",
    "        Returns min_dist, the single linkage distance between two clusters cluster1 and cluster2.\n",
    "        \n",
    "        The single linkage distance between two clusters is the minimum distance between any two points in the two clusters.\n",
    "        '''\n",
    "\n",
    "        min_dist = 1e9\n",
    "        for i in cluster1:\n",
    "            for j in cluster2:\n",
    "                min_dist = min(min_dist, self.distances[i][j])\n",
    "        return min_dist\n",
    "    \n",
    "    def complete_linkage(self, cluster1, cluster2):\n",
    "        '''Returns max_dist, the complete linkage distance between two clusters cluster1 and cluster2.\n",
    "        \n",
    "        The complete linkage distance between two clusters is the maximum distance between any two points in the two clusters.\n",
    "        '''\n",
    "        max_dist = 0\n",
    "        for i in cluster1:\n",
    "            for j in cluster2:\n",
    "                max_dist = max(max_dist, self.distances[i][j])\n",
    "        return max_dist\n",
    "    \n",
    "    def split_point(self, cluster):\n",
    "        '''\n",
    "        Returns extra_point about which cluster is to be split.\n",
    "        \n",
    "        The function choose the point which is most distant from the cluster, which is determined by calculating the average\n",
    "        distance of each point in the cluster from all the other points in the cluster. The point which has the maximum average\n",
    "        distance is returned.\n",
    "        '''\n",
    "\n",
    "        max_avg_dist = 0\n",
    "        extra_point = -1\n",
    "        for i in cluster:\n",
    "            avg_dist = 0\n",
    "            for j in cluster:\n",
    "                avg_dist += self.distances[i][j]\n",
    "            avg_dist /= len(cluster)\n",
    "            if avg_dist > max_avg_dist:\n",
    "                max_avg_dist = avg_dist\n",
    "                extra_point = i\n",
    "        return extra_point\n",
    "    \n",
    "    def split_cluster(self, cluster, extra_point):\n",
    "        '''\n",
    "        Returns two clusters obtained by splitting cluster about extra_point.\n",
    "        \n",
    "        The function splits the cluster about the extra_point by calculating the single linkage distance of every point in the\n",
    "        original cluster other than extra point between the original cluster (without the point) and the new cluster, which\n",
    "        initially containing only the extra_point. If the point's single linkage distance is lesser with the new cluster, it \n",
    "        is added to the new cluster, otherwise it is added back to the original cluster.\n",
    "        '''\n",
    "\n",
    "        cluster2 = cluster\n",
    "        cluster2.remove(extra_point)\n",
    "        cluster1 = [extra_point]\n",
    "        for i in cluster:\n",
    "            if i in cluster1:\n",
    "                continue\n",
    "            cluster2.remove(i)\n",
    "            if self.single_linkage(cluster1, [i]) < self.single_linkage(cluster2, [i]):\n",
    "                cluster1.append(i)\n",
    "            else:\n",
    "                cluster2.append(i)\n",
    "        return cluster1, cluster2\n",
    "\n",
    "    def fit(self, X):\n",
    "        '''\n",
    "        Returns clusters and cluster_indices after performing the clustering on the input data X.\n",
    "        \n",
    "        The function precomputes the pairwise distances between all points in X. It then iteratively splits until each point is\n",
    "        its own cluster. It does this by choosing the cluster with the largest diameter and with more than one point, finding the \n",
    "        point with the largest average dissimilarity and then split the cluster about this point. It then uses complete linkage on\n",
    "        the clusters to merge the clusters which are the closest among all pairwise clusters until k clusters are obtained.\n",
    "        '''\n",
    "\n",
    "        self.X = X\n",
    "        self.clusters = [[i for i in range(len(X))]]\n",
    "        self.distances = np.zeros((len(X), len(X)))\n",
    "        for i in range(len(X)):                                                          # compute distance\n",
    "            for j in range(i+1, len(X)):\n",
    "                self.distances[i][j] = self.cosine_similarity_distance(X[i], X[j])\n",
    "                self.distances[j][i] = self.distances[i][j]\n",
    "        while len(self.clusters) < len(self.X):                                          # iteration\n",
    "            cluster_to_split = -1                                                        # find cluster to split\n",
    "            if len(self.clusters) == 1:\n",
    "                cluster_to_split = 0\n",
    "            else:\n",
    "                max_diameter = 0\n",
    "                for i in range(len(self.clusters)):\n",
    "                    if len(self.clusters[i]) == 1:\n",
    "                        continue\n",
    "                    if self.cluster_diameter(self.clusters[i]) > max_diameter:\n",
    "                        max_diameter = self.cluster_diameter(self.clusters[i])\n",
    "                        cluster_to_split = i\n",
    "            extra_point = self.split_point(self.clusters[cluster_to_split])              # find point to split the cluster\n",
    "            #print('Extra point is {}'.format(extra_point))\n",
    "            if len(self.clusters[cluster_to_split]) == 2:                                # split directly if only two points\n",
    "                self.clusters[cluster_to_split].remove(extra_point)\n",
    "                self.clusters.append([extra_point])\n",
    "                continue\n",
    "            cluster1, cluster2 = self.split_cluster(self.clusters[cluster_to_split], extra_point)     # split the cluster\n",
    "            self.clusters[cluster_to_split] = cluster1                                                # update the new clusters\n",
    "            self.clusters.append(cluster2)                                                  \n",
    "            \n",
    "        while len(self.clusters) > self.k:                                                            # create k clusters using complete linkage\n",
    "            min_dist = 1e9\n",
    "            cluster1 = -1\n",
    "            cluster2 = -1\n",
    "            for i in range(len(self.clusters)):\n",
    "                for j in range(i+1, len(self.clusters)):\n",
    "                    if self.complete_linkage(self.clusters[i], self.clusters[j]) < min_dist:\n",
    "                        min_dist = self.complete_linkage(self.clusters[i], self.clusters[j])\n",
    "                        cluster1 = i\n",
    "                        cluster2 = j\n",
    "            self.clusters[cluster1].extend(self.clusters[cluster2])\n",
    "            self.clusters.remove(self.clusters[cluster2])\n",
    "        self.cluster_indices = [0 for i in range(len(X))]\n",
    "        for i in range(len(self.clusters)):\n",
    "            for j in self.clusters[i]:\n",
    "                self.cluster_indices[j] = i\n",
    "        return self.clusters, self.cluster_indices\n",
    "    \n",
    "    def silhouette_score(self):\n",
    "        '''\n",
    "        Evaluates the clustering using the silhouette score metric after the clustering has been done using fit().\n",
    "\n",
    "        List a is evaluated as the average distance between a point and all the other points in the same cluster,\n",
    "        taking care of the null case by using mean as 0. List b is evaluated as the average distance between a point\n",
    "        and all the other points in the nearest cluster, which is decided by finding cluster with minimum average \n",
    "        distance to the point.\n",
    "         \n",
    "        The silhouette score list s_arr is then calculated as (b - a) / max(a, b) for each point and the final score\n",
    "        s is the mean of all the values in s_arr.\n",
    "        '''\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(self.X)):\n",
    "            ai = np.array([self.cosine_similarity_distance(self.X[i], self.X[j]) for j in range(len(self.X)) if i != j and self.cluster_indices[i] == self.cluster_indices[j]])\n",
    "            a.append(np.mean(ai) if len(ai) > 0 else 0)\n",
    "            if np.isnan(a[-1]):\n",
    "                a[-1] = 1e9\n",
    "    \n",
    "        for i in range(len(self.X)):\n",
    "            bi = []\n",
    "            for n in range(self.k):\n",
    "                if n == self.cluster_indices[i]:\n",
    "                    continue\n",
    "                temp = [self.cosine_similarity_distance(self.X[i], self.X[j]) for j in range(len(self.X)) if i != j and self.cluster_indices[j] == n]\n",
    "                if len(temp) == 0:\n",
    "                    temp = [1e9]\n",
    "                bi.append(np.mean(temp))\n",
    "            b.append(min(bi))\n",
    "            if np.isnan(b[-1]):\n",
    "                b[-1] = 1e9\n",
    "        \n",
    "        s_arr = [(b[i] - a[i]) / (max(a[i], b[i])) for i in range(len(self.X))]\n",
    "        s = np.mean(s_arr)\n",
    "        return s\n",
    "    \n",
    "    def save_file(self, file_name):  \n",
    "        '''\n",
    "        Saves the clusters in a file with the name file_name.\n",
    "        \n",
    "        Each individual cluster is sorted and then the clusters are sorted according to the first element of each cluster\n",
    "        (which gives the minimum indexed point in the cluster). These clusters are then saved in the file, with each cluster\n",
    "        on a new line and all the cluster indices in the cluster separated by a comma.\n",
    "        '''\n",
    "\n",
    "        temp_clusters = [sorted(self.clusters[i]) for i in range(len(self.clusters))]\n",
    "        min_vals = [[temp_clusters[i][0], i] for i in range(len(temp_clusters))]\n",
    "        min_vals = sorted(min_vals, key=lambda x: x[0])\n",
    "        final_clusters = []\n",
    "        for i in range(len(temp_clusters)):\n",
    "            final_clusters.append(temp_clusters[min_vals[i][1]])\n",
    "        with open(file_name, 'w') as f:\n",
    "            for i in range(len(final_clusters)):\n",
    "                for j in range(len(final_clusters[i])):\n",
    "                    f.write(str(final_clusters[i][j]) + ', ')\n",
    "                f.write('\\n')\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for k = 3 is 0.4544496561732083\n"
     ]
    }
   ],
   "source": [
    "best_model_hdc = None\n",
    "best_score_hdc = -1\n",
    "myHDC = singleLinkageTopDown(best_k)\n",
    "clusters = myHDC.fit(X)\n",
    "score = myHDC.silhouette_score()\n",
    "print('Silhouette score for k = {} is {}'.format(best_k, score))\n",
    "if score > best_score_hdc:\n",
    "    best_score_hdc = score\n",
    "    best_model_hdc = myHDC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacard similarity matrix is\n",
      "[0.07142857142857142, 0.847457627118644, 0.0]\n",
      "[0.03614457831325301, 0.0, 0.7608695652173914]\n",
      "[0.6551724137931034, 0.02040816326530612, 0.0963855421686747]\n"
     ]
    }
   ],
   "source": [
    "# Get the clusters and the cluster indices from the models\n",
    "hdc_clusters = best_model_hdc.clusters\n",
    "hdc_cluster_indices = best_model_hdc.cluster_indices\n",
    "kmeans_clusters = best_model_kmeans.clusters\n",
    "kmeans_cluster_indices = best_model_kmeans.cluster_indices\n",
    "\n",
    "# find pairwise jacard similarity between the clusters\n",
    "# jacard_matrix[i][j] = jacard similarity between hdc_clusters[i] and kmeans_clusters[j]\n",
    "jacard_matrix = [[0 for i in range(best_k)] for j in range(best_k)]\n",
    "for i in range(best_k):\n",
    "    for j in range(best_k):\n",
    "        set_1 = set(hdc_clusters[i])\n",
    "        set_2 = set(kmeans_clusters[j])\n",
    "        jacard_matrix[i][j] = len(set_1.intersection(set_2)) / len(set_1.union(set_2))\n",
    "\n",
    "\n",
    "print('Jacard similarity matrix is')\n",
    "for i in range(best_k):\n",
    "    print(jacard_matrix[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping and Jacard similarities are:\n",
      "0 -> 1 with jacard similarity 0.847457627118644\n",
      "1 -> 2 with jacard similarity 0.7608695652173914\n",
      "2 -> 0 with jacard similarity 0.6551724137931034\n"
     ]
    }
   ],
   "source": [
    "# create mapping between clusters\n",
    "# mapping[i] = j means hdc_clusters[i] is mapped to kmeans_clusters[j]\n",
    "# a one-one and onto mapping is created by choosing best jaccard similarity for each kmeans cluster with the hdc cluster\n",
    "# note that in some cases a kmeans cluster might not get the highest jacard similarity as a mapping as it has been mapped to another cluster already\n",
    "mapping = {}\n",
    "for i in range(best_k):\n",
    "    max_jacard = 0\n",
    "    max_jacard_index = -1\n",
    "    possible_indices = [j for j in range(best_k) ]\n",
    "    for j in range(best_k):\n",
    "        if j in mapping.keys():\n",
    "            possible_indices.remove(mapping[j])\n",
    "    for j in possible_indices:\n",
    "        if jacard_matrix[i][j] > max_jacard:\n",
    "            max_jacard = jacard_matrix[i][j]\n",
    "            max_jacard_index = j\n",
    "    mapping[i] = max_jacard_index\n",
    "\n",
    "print('Mapping and Jacard similarities are:')\n",
    "for i in range(best_k):\n",
    "    print('{} -> {} with jacard similarity {}'.format(i, mapping[i], jacard_matrix[i][mapping[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_kmeans.save_file('kmeans.txt')\n",
    "best_model_hdc.save_file('divisive.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
